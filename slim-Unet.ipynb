{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os, sys\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/shou/network/tf-models/research/slim\")\n",
    "sys.path.append(\"/home/shou/network/fcn/tf-image-segmentation\")\n",
    "from tf_image_segmentation.utils.augmentation import (distort_randomly_image_color,\n",
    "                                                      flip_randomly_left_right_image_with_annotation,\n",
    "                                                      scale_randomly_image_with_annotation_with_fixed_size_output)\n",
    "from tf_image_segmentation.utils.pascal_voc import pascal_segmentation_lut\n",
    "\n",
    "from tf_image_segmentation.utils.training import get_valid_logits_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def u_net(image):\n",
    "    with tf.variable_scope(\"u_net\", reuse=None):\n",
    "        with slim.arg_scope([slim.conv2d,  slim.conv2d_transpose, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\n",
    "            # slim.conv2d default relu activation\n",
    "            # subsampling\n",
    "            conv0 = slim.repeat(image, 2, slim.conv2d, 32, [3, 3], scope='conv0')\n",
    "            pool0 = slim.max_pool2d(conv0, [2, 2], scope='pool0')  # 1/2\n",
    "            bn0 = slim.batch_norm(pool0, decay=0.9, epsilon=1e-5, scope=\"bn0\")\n",
    "            \n",
    "            conv1 = slim.repeat(bn0, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "            pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')  # 1/4\n",
    "            bn1 = slim.batch_norm(pool1, decay=0.9, epsilon=1e-5, scope=\"bn1\")\n",
    "            \n",
    "            conv2 = slim.repeat(bn1, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool2')  # 1/8\n",
    "            bn2 = slim.batch_norm(pool2, decay=0.9, epsilon=1e-5, scope=\"bn2\")\n",
    "            \n",
    "            conv3 = slim.repeat(bn2, 2, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "            pool3 = slim.max_pool2d(conv3, [2, 2], scope='pool3')  # 1/16\n",
    "            bn3 = slim.batch_norm(pool3, decay=0.9, epsilon=1e-5, scope=\"bn3\")\n",
    "            \n",
    "            conv4 = slim.repeat(bn3, 2, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "            pool4 = slim.max_pool2d(conv4, [2, 2], scope='pool4')  # 1/32\n",
    "            bn4 = slim.batch_norm(pool4, decay=0.9, epsilon=1e-5, scope=\"bn4\")\n",
    "            \n",
    "            # upsampling\n",
    "            conv_t1 = slim.conv2d_transpose(bn4, 256, [2,2], scope='conv_t1') # up to 1/16 + conv3\n",
    "            merge1 = tf.concat([conv_t1, conv3], 3)\n",
    "            conv5 = slim.stack(merge1, slim.conv2d, [(512, [3, 3]),(256, [3,3])], scope='conv5')\n",
    "            bn5 = slim.batch_norm(conv5, decay=0.9, epsilon=1e-5, scope='bn5')\n",
    "            \n",
    "            conv_t2 = slim.conv2d_transpose(bn5, 128, [2,2], scope='conv_t2') # up to 1/8 + conv2\n",
    "            merge2 = tf.concat([conv_t2, conv2], 3)\n",
    "            conv6 = slim.stack(merge2, slim.conv2d, [(256, [3,3]), (128, [3,3])], scope='conv6')\n",
    "            bn6 = slim.batch_norm(conv6, decay=0.9, epsilon=1e-5, scope='bn6')\n",
    "            \n",
    "            conv_t3 = slim.conv2d_transpose(bn6, 64, [2,2], scope='conv_t3') # up to 1/4 + conv1\n",
    "            merge3 = tf.concat([conv_t3, conv1], 3)\n",
    "            conv7 = slim.stack(merge3, slim.conv2d, [(128, [3,3]), (64, [3,3])], scope='conv7')\n",
    "            bn7 = slim.batch_norm(conv7, decay=0.9, epsilon=1e-5, scope='bn7')\n",
    "            \n",
    "            conv_t4 = slim.conv2d_transpose(bn7, 32, [2,2], scope='convt4')  # up to 1/2 + conv0\n",
    "            merge4 = tf.concat([conv_t4, conv0], 3)\n",
    "            conv8 = slim.stack(merge4, slim.conv2d, [(64, [3,3]), (32, [3,3])], scope='conv8')\n",
    "            bn8 = slim.batch_norm(conv7, decay=0.9, epsilon=1e-5, scope='bn8')\n",
    "            \n",
    "            # output layer scoreMap\n",
    "            conv9 = slim.conv2d(bn7, 1, [1,1], scope='scoreMap') # 2 CLASSES_NUM\n",
    "            annotation_pred = tf.argmax(conv9, dimension=3, name='prediction')\n",
    "            return annotation_pred, conv9\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tfrecord_and_decode_into_image_annotation_pair_tensors(tf_filenames_queue):\n",
    "    \"\"\"Return image/annotation tensors that are created by reading tfrecord file.\n",
    "    The function accepts tfrecord filenames queue as an input which is usually\n",
    "    can be created using tf.train.string_input_producer() where filename\n",
    "    is specified with desired number of epochs. This function takes queue\n",
    "    produced by aforemention tf.train.string_input_producer() and defines\n",
    "    tensors converted from raw binary representations into\n",
    "    reshaped image/annotation tensors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tfrecord_filenames_queue : tfrecord filename queue\n",
    "        String queue object from tf.train.string_input_producer()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    image, annotation : tuple of tf.int32 (image, annotation)\n",
    "        Tuple of image/annotation tensors\n",
    "    \"\"\"\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(tf_filenames_queue)\n",
    "    \n",
    "    features = tf.parse_single_example(serialized_example, \n",
    "                                      features={\n",
    "                                          'height':tf.FixedLenFeature([], tf.int64),\n",
    "                                          'width':tf.FixedLenFeature([], tf.int64),\n",
    "                                          'image_raw':tf.FixedLenFeature([], tf.string),\n",
    "                                          'mask_raw':tf.FixedLenFeature([], tf.string)\n",
    "                                      })\n",
    "    \n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "    annotation = tf.decode_raw(features['mask_raw'], tf.uint8)\n",
    "    \n",
    "    height = tf.cast(features['height'], tf.int32)\n",
    "    width = tf.cast(features['width'], tf.int32)\n",
    "    \n",
    "    image_shape = tf.stack([height, width, 3])\n",
    "    annotation_shape = tf.stack([height, width, 1])\n",
    "    \n",
    "    image = tf.reshape(image, image_shape)\n",
    "    annotation = tf.reshape(annotation, annotation_shape)\n",
    "    \n",
    "    return image, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_train_size = [384, 384]\n",
    "number_of_classes = 2\n",
    "pascal_voc_lut = pascal_segmentation_lut(number_of_classes)\n",
    "class_labels = pascal_voc_lut.keys()\n",
    "tfrecord_filename = '/home/shou/network/dataset/pascal_augmented_train_island.tfrecords'\n",
    "filename_queue = tf.train.string_input_producer([tfrecord_filename], num_epochs=5)\n",
    "\n",
    "image, annotation = read_tfrecord_and_decode_into_image_annotation_pair_tensors(filename_queue)\n",
    "image, annotation = flip_randomly_left_right_image_with_annotation(image, annotation)\n",
    "resized_image, resized_annotation = scale_randomly_image_with_annotation_with_fixed_size_output(image, annotation, image_train_size)\n",
    "resized_annotation = tf.squeeze(resized_annotation)\n",
    "\n",
    "image_batch, annotation_batch = tf.train.shuffle_batch( [resized_image, resized_annotation],\n",
    "                                             batch_size=1,\n",
    "                                             capacity=3000,\n",
    "                                             num_threads=2,\n",
    "                                             min_after_dequeue=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_for_train = tf.placeholder(tf.float32, shape=[486, 384, 384, 3], name=\"input_image\")\n",
    "annotation_for_train = tf.placeholder(tf.float32, shape=[486, 384, 384, 1], name=\"annotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_annotation, logits = u_net(image_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropies =tf.losses.sparse_softmax_cross_entropy(logits=logits,\n",
    "                                                           labels=tf.squeeze(annotation_batch)\n",
    "                                                           )\n",
    "cross_entropy_sum = tf.reduce_mean(cross_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"adam_vars\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=1e-5).minimize(cross_entropies)\n",
    "\n",
    "global_vars_init_op = tf.global_variables_initializer()\n",
    "local_vars_init_op = tf.local_variables_initializer()\n",
    "combined_op = tf.group(global_vars_init_op, local_vars_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    image_batch_float = sess.run(image_batch)\n",
    "    image_annotation_float = sess.run(annotation_batch)\n",
    "    feed_dict1 = {image_for_train:image_batch_float, annotation_for_train:image_annotation_float}\n",
    "    sess.run(combined_op, feed_dict=feed_dict1)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    for i in xrange(5*486):\n",
    "        cross_entropy, _ = sess.run([cross_entropy_sum, train_step], feed_dict=feed_dict1)\n",
    "        print(str(i) + \" Current loss: \" + str(cross_entropy))\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
